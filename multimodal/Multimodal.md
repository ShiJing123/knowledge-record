# Recommended Papers ![Maintenance](https://img.shields.io/maintenance/yes/2017.svg) [![DUB](https://img.shields.io/dub/l/vibe-d.svg)](LICENSE)
## [Home](../README.md)
- The goal of this document is to provide a reading list in Multimodal learning.


## Topics
- [Image Captioning](#image-captioning)
- [Video Descriptions](#video-descriptions)
- [Visual Question-Answer (VQA)](#visual-question-answer)
- [Human Pose Estimation](#human_pose_estimation)
- [Medical](medical/medical.md)
- [Others](#others)
- [Multimodal Machine Learning](https://sites.google.com/site/multiml2016cvpr/)

## Five main __challenges__ in multimodal machine learning
-  multimodal representation learning
-  translation & mapping
-  modality alignment
-  multimodal fusion 
-  co-learning

## Papers
Paper list.

## Image Captioning
|No.  |Figure   |Title   |Authors  |Pub.  |Links|Datasets|
|-----|:-----:|:-----:|:-----:|:-----:|:---:|:---:|
|1|![Smile](paper_image/Show-and-Tell.png)|__Show and tell: A neural image caption generator__| [Oriol Vinyals](https://research.google.com/pubs/OriolVinyals.html), Alexander Toshev, [Samy Bengio](http://bengio.abracadoudou.com/), Dumitru Erhan|__CVPR 2015__|[Smile](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43274.pdf) <br/>解读| [COCO](http://cocodataset.org/#home)|
|2|![Smile](paper_image/show_and_tell_architecture.png)|__Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge__| [Oriol Vinyals](https://research.google.com/pubs/OriolVinyals.html), Alexander Toshev, Samy Bengio, Dumitru Erhan|__PAMI 2016__|[PDF](https://arxiv.org/abs/1609.06647) [github](https://github.com/tensorflow/models/tree/master/research/im2txt) 解读|[COCO](http://cocodataset.org/#home)|
|3|![Smile](paper_image/Jointly-Learning-Energy-Expenditures-and-Activities-using-Egocentric-Multimoda.png)|__Attend to You: Personalized Image Captioning with Context Sequence Memory Networks__|Cesc Chunseong Park, Byeongchang Kim and Gunhee Kim|__CVPR 2017__|[PDF](https://arxiv.org/abs/1704.06485) [code](https://github.com/cesc-park/attend2u) 解读|[IntaPIC-1.1M Json](https://drive.google.com/uc?export=download&id=0B3xszfcsfVUBdG0tU3BOQWV0a0E) [IntaPIC-1.1M Image](https://drive.google.com/ucexport=download&id=0B3xszfcsfVUBVkZGU2oxYVl6aDA)|
|4|![Smile](paper_image/Dense-Captioning-Events-in-Videos.png)|__Dense-Captioning Events in Videos__|Ranjay Krishna, Kenji Hata, Frederic Ren, [Fei-Fei, Li](http://vision.stanford.edu/publications.html#year2017), Juan Carlos Niebles|__CVPR 2017__|[PDF](https://arxiv.org/abs/1705.00754) [Project](http://cs.stanford.edu/people/ranjaykrishna/densevid/) 解读|[ActivityNet Captions](http://cs.stanford.edu/people/ranjaykrishna/densevid/captions.zip)|


## Video Descriptions

### code
[视频内容问答](https://www.leiphone.com/news/201705/qtBJ3lsLVJ1TPrwK.html)

##Visual Question-Answer

## Human Pose Estimation
|No.  |Figure   |Title   |Authors  |Pub.  |Links|Datasets|
|-----|:-----:|:-----:|:-----:|:-----:|:---:|:---:|
|1|![Smile](paper_image/Show-and-Tell.png)|__MODEC: Multimodal Decomposable Models for Human Pose Estimation
__| Ben Sapp, Ben Taskar|__CVPR 2013__|[PDF](https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sapp_MODEC_Multimodal_Decomposable_2013_CVPR_paper.pdf) <br/>解读| 

## Others
|No.  |Figure   |Title   |Authors  |Pub.  |Links| Datasets |
|-----|:-----:|:-----:|:-----:|:-----:|:---:|:---:|
|1|![Smile](image/MDF.png)|__Jointly Learning Energy Expenditures and Activities using Egocentric Multimoda__|Nakamura, Katsuyuki; Yeung, Serena; Alahi, Alexandre; [Fei-Fei, Li](http://vision.stanford.edu/publications.html#year2017)|__CVPR 2017__|[PDF](http://vision.stanford.edu/pdf/nakamura2017cvpr.pdf) 解读||

